{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drYEhVJcLLS3"
   },
   "source": [
    "# This cat does not exist\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "Цель этого задания - создать котов, которых не существует. В ходе данного задания вы обучите DCGAN и VAE, которые являются одними из первых генеративных моделей. Для этого задания вам наверняка потребуется GPU с CUDA, поэтому рекомендуется использовать Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivL-X-OhLLS6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFxnL6uhLLS8"
   },
   "outputs": [],
   "source": [
    "def random_noise(batch_size, channels, side_size):\n",
    "    return torch.randn(batch_size, channels, side_size, side_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j47U0kZGLLS8"
   },
   "outputs": [],
   "source": [
    "def visualise(imgs, rows=2):\n",
    "    imgs = (imgs.transpose(1, 3) + 1) / 2\n",
    "    imgs = torch.cat([imgs[i::rows] for i in range(rows)], dim=1)\n",
    "    cols = len(imgs)\n",
    "    imgs = (torch.cat(list(imgs), dim=1)).cpu().numpy()[:, :, ::-1]\n",
    "    plt.figure(figsize=(cols*1.5, rows*1.5))\n",
    "    plt.imshow(imgs)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ2thNUcLLS9"
   },
   "outputs": [],
   "source": [
    "class CatDataset(Dataset):\n",
    "    def __init__(self, path_to_dataset=\"cat_136\", size=64):\n",
    "        self.photo_names = os.listdir(path_to_dataset)\n",
    "        self.path_base = path_to_dataset\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.path_base + \"/\" + self.photo_names[index]\n",
    "        img = cv2.imread(path) # 136 x 136\n",
    "        crop_rate = 8\n",
    "        x_crop = random.randint(0, crop_rate)\n",
    "        y_crop = random.randint(0, crop_rate)\n",
    "        img = img[x_crop:136 - crop_rate + x_crop, y_crop:136 - crop_rate + y_crop]\n",
    "        img = cv2.resize(img, (self.size, self.size), interpolation=cv2.INTER_CUBIC)\n",
    "        return 2 * torch.tensor(img).float().transpose(0, 2) / 255. - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.photo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpjrw6A3LLS9"
   },
   "outputs": [],
   "source": [
    "dataset = CatDataset()\n",
    "visualise(torch.cat([dataset[i].unsqueeze(0) for i in [3, 15, 182, 592, 394, 2941]], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXv8hAXULLS-"
   },
   "source": [
    "### Задание 1 (2 балла)\n",
    "Для начала реализуем кодировщик для нашего VAE. Предлагается использовать следующую архитектуру:\n",
    "\n",
    "![](imgs/VAE_enc.png)\n",
    "\n",
    "Для ее реализации вам потребуются модули `nn.Conv2d`, `nn.ReLU`, `nn.Flatten` и `nn.Linear`.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `latent_size`, `start_channels` и `downsamplings`. Первый аргумент - высота и ширина картинки в пикселях. Второй аргумент - размерность латентного пространства. `start_channels` отвечает за то, сколько каналов должно быть в картинке перед тем, как к ней будут применены downsampling блоки. `downsamplings` - это количество downsampling блоков, которые должны быть применены к картинке. В каждом таком блоке количество каналов увеличивается в два раза.\n",
    "\n",
    "\n",
    "* `forward` - принимает на вход батч `x`, возвращает эмбеддинг в латентном пространстве `z` и параметры распределения `(mu, sigma)`.\n",
    "\n",
    "\n",
    "#### Важно\n",
    "`Linear Model` в схеме - это полносвязная сеть из нескольких слоев. Предлагается такая архитектура:\n",
    "\n",
    "`[Linear(c * img_size // 2^N -> 256), ReLU, Linear(256 -> 2 * latent_size)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG1-PV6oLLS_"
   },
   "outputs": [],
   "source": [
    "from task import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0UM3HHpLLS_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8up80_Z-LLTA"
   },
   "source": [
    "### Задание 2 (3 балла)\n",
    "Теперь реализуем декодер для VAE. Предлагается использовать следующую архитектуру:\n",
    "\n",
    "![](imgs/VAE_dec.png)\n",
    "\n",
    "Для ее реализации вам потребуются модули `nn.Linear`, `nn.Unflatten`, `nn.ConvTranspose2d`, `nn.ReLU` и `nn.Tanh`.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `latent_size`, `end_channels` и `upsamplings`.  Первый аргумент - высота и ширина картинки в пикселях. Второй аргумент - размерность латентного пространства. `end_channels` отвечает за то, сколько каналов должно быть после всех upsampling блоков. `upsamplings` - это количество upsampling блоков, которые должны быть применены к картинке. В каждом таком блоке количество каналов уменьшается в два раза.\n",
    "\n",
    "\n",
    "* `forward` - принимает на вход `z` - тензор с латентным представлением изображений. Возвращает батч восстановленных изображений.\n",
    "\n",
    "#### Важно\n",
    "`Linear Model` в схеме - это полносвязная сеть из нескольких слоев. Предлагается такая архитектура:\n",
    "\n",
    "`[Linear(latent_size -> 256), ReLU, Linear(256 -> c * 2^N)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7I_Ry3CLLTB"
   },
   "outputs": [],
   "source": [
    "from task import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeFjX9cwLLTB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Huqptx5LLTC",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46M_oEWULLTC"
   },
   "source": [
    "### Задание 3 (5 баллов)\n",
    "Наконец, объединим Encoder и Decoder в Variational Autoencoder.\n",
    "\n",
    "Полностью архитектура выглядит так:\n",
    "![](imgs/VAE.png) \n",
    "\n",
    "Из нее можно выделить две части: Encoder (по изображению возвращает mu и sigma) и Decoder (по случайному шуму восстанавливает изображение). На высоком уровне VAE можно представить так:\n",
    "\n",
    "![](imgs/VAE_highlevel.png)\n",
    "\n",
    "В данном задании вам необходимо реализовать полную архитектуру VAE.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `downsamplings`, `latent_size`, `linear_hidden_size`, `down_channels` и `up_channels`. `img_size` - размер стороны входного изображения. `downsamplings` - количество downsampling (и upsampling) блоков. `latent_size` - размер латентного пространства, в котором в который будет закодирована картинка. `linear_hidden_size` количество нейронов на скрытом слое полносвязной сети в конце encoder'а. Для полносвязной сети decoder'а это число стоит умножить на 2. `down_channels` - количество каналов, в которое будет преобразовано трехцветное изображение перед применением `downsampling` блоков. `up_channels` - количество каналов, которое должно получиться после применения всех upsampling блоков.\n",
    "\n",
    "* `forward` - принимает на вход `x`. Считает распределение $N(\\mu, \\sigma^2)$ и вектор $z \\sim N(\\mu, \\sigma^2)$. Возвращает $x'$ - восстановленную из вектора $z$ картинку и $D_{KL}(N(\\mu, \\sigma^2), N(0, 1)) = 0.5 \\cdot (\\sigma^2 + \\mu^2 - \\log \\sigma^2 - 1)$.\n",
    "\n",
    "* `encode` - принимает на вход `x`. Возвращает вектор из распределения $N(\\mu, \\sigma^2)$.\n",
    "\n",
    "* `decode` - принимает на вход `z`. Возвращает восстановленную по вектору картинку.\n",
    "\n",
    "* `save` - сохраняет чекпоинт обученной модели.\n",
    "\n",
    "* `load` - загружает сохраненный чекпоинт обученной модели.\n",
    "\n",
    "\n",
    "#### Если хочется улучшить качество\n",
    "https://arxiv.org/pdf/1906.00446.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtW6VpF4LLTC"
   },
   "outputs": [],
   "source": [
    "from task import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SXjsy_qLLTD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDfUzcbGLLTD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_vae():\n",
    "    vae = VAE()\n",
    "    vae.cuda()\n",
    "\n",
    "    epochs = 201\n",
    "    batch_size = 8\n",
    "    vae_optim = Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "    dataset = CatDataset(size=128)\n",
    "\n",
    "    test_imgs_1 = torch.cat([dataset[i].unsqueeze(0) for i in (0, 34, 76, 1509)])\n",
    "    test_imgs_2 = torch.cat([dataset[i].unsqueeze(0) for i in (734, 123, 512, 3634)])\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "        total_batches = 0\n",
    "        rec_loss_avg = 0\n",
    "        kld_loss_avg = 0\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                z_1 = vae.encode(test_imgs_1.cuda())\n",
    "                z_2 = vae.encode(test_imgs_2.cuda())\n",
    "                x_int = []\n",
    "                for i in range(9):\n",
    "                    z = (i * z_1 + (8 - i) * z_2) / 8\n",
    "                    x_int.append(vae.decode(z))\n",
    "                x_int = torch.cat(x_int)\n",
    "                visualise(x_int, rows=len(test_imgs_1))\n",
    "                z_rand = torch.randn_like(z_1)\n",
    "                x_int = vae.decode(z_rand)\n",
    "                visualise(x_int, rows=len(test_imgs_1)//2)\n",
    "\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) // batch_size):\n",
    "            if len(batch) < batch_size:\n",
    "                continue\n",
    "            total_batches += 1\n",
    "            x = batch.cuda()\n",
    "            x_rec, kld = vae(x)\n",
    "            img_elems = float(np.prod(list(batch.size())))\n",
    "            kld_loss = kld.sum() / batch_size\n",
    "            rec_loss = ((x_rec - x)**2).sum() / batch_size\n",
    "            loss = rec_loss + 0.1 * kld_loss # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            vae_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            vae_optim.step()\n",
    "            kld_loss_avg += kld_loss.item()\n",
    "            rec_loss_avg += rec_loss.item()\n",
    "\n",
    "        print(f\"Epoch {ep+1} | Reconstruction loss: {rec_loss_avg / total_batches} | KLD loss: {kld_loss_avg / total_batches}\")\n",
    "\n",
    "train_vae()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw06_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
