{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GeqjUsgn4HY"
   },
   "source": [
    "# Линейная регрессия\n",
    "__Суммарное количество баллов: 10__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmYMrZJGn4Hg"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l86n1A9Bn4Hj"
   },
   "outputs": [],
   "source": [
    "def read_data(path=\"boston.csv\"):\n",
    "    dataframe = np.genfromtxt(path, delimiter=\",\", skip_header=15)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(dataframe)\n",
    "    X = dataframe[:, :-1]\n",
    "    y = dataframe[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIDuGR68n4Hj"
   },
   "outputs": [],
   "source": [
    "def generate_synthetic(size:int, dim=6, noise=0.1):\n",
    "    X = np.random.randn(size, dim)\n",
    "    w = np.random.randn(dim + 1)\n",
    "    noise = noise * np.random.randn(size)\n",
    "    y = X.dot(w[1:]) + w[0] + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTQrXJM3n4Hk"
   },
   "source": [
    "### Задание 1 (1 балл)\n",
    "Для начала нужно понять, какую метрику для ошибки будем использовать. В нашем случае нам подойдет стандартная метрика MSE. Также чтобы оценить качество модели нам понадобится метрика $R^2$. Реализуйте обе эти метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhFj_POqn4Hl"
   },
   "outputs": [],
   "source": [
    "from task import mse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl6-3k-wn4Hm"
   },
   "source": [
    "### Задание 2 (3 балла)\n",
    "Теперь реализуем линейную регрессию при помощи явного решения задачи минимизации. \n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - решает задачу минимизации $\\arg\\min_{w, b}\\sum ((w\\cdot x + b) - y)^2$. \n",
    "\n",
    "`predict(X)` - строит предсказание `y` для объектов из `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ5qQ1p3n4Hn"
   },
   "outputs": [],
   "source": [
    "from task import NormalLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qg5BqJPAn4Hn"
   },
   "outputs": [],
   "source": [
    "X, y = generate_synthetic(1024)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyB5sun8n4Ho"
   },
   "outputs": [],
   "source": [
    "regr = NormalLR()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "print(f\"MSE: {mse(y_test, y_pred)}, R2: {r2(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ9rFp-gn4Hp"
   },
   "source": [
    "### Задание 3 (4 балла)\n",
    "Теперь реализуем линейную регрессию с использованием градиентного спуска с larning rate `alpha` в течении `iterations` итераций. В задании необходимо использовать регуляризацию Лассо с коэффициентом `l`.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - приближает решение задачи минимизации $\\arg\\min_{w, b}\\sum ((w\\cdot x + b) - y)^2$ при помощи градиентного спуска. \n",
    "\n",
    "\n",
    "`predict(X)` - строит предсказание `y` для объектов из `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzw2-dcn4Hq"
   },
   "outputs": [],
   "source": [
    "from task import GradientLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcfjGBREn4Hq"
   },
   "outputs": [],
   "source": [
    "def build_plot(X_train, y_train, X_test, y_test):\n",
    "    xs = np.arange(0.0, 0.02, 0.0002)\n",
    "    errors = []\n",
    "    for x in xs:\n",
    "        regr = GradientLR(0.1, iterations=10000, l=x)\n",
    "        regr.fit(X_train, y_train)\n",
    "        errors.append(mse(y_test, regr.predict(X_test)))\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.xlim(xs[0], xs[-1])\n",
    "    plt.grid()\n",
    "    plt.plot(xs, errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ8txzZdn4Hr"
   },
   "outputs": [],
   "source": [
    "X, y = generate_synthetic(1024)\n",
    "X, X_val, y, y_val = train_test_split(X, y, train_size=0.9, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7QFa1czn4Hs"
   },
   "outputs": [],
   "source": [
    "build_plot(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siP8OKLRn4Ht"
   },
   "outputs": [],
   "source": [
    "regr = GradientLR(0.1, iterations=10000)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "print(f\"MSE: {mse(y_test, y_pred)}, R2: {r2(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (2 балла)\n",
    "Линейная регрессия является методом, который можно интерпретировать основываясь на значениях весов модели. Реализуйте метод, который будет упорядочивать признаки по их важности от наибольшей важности к наименьшей.\n",
    "\n",
    "Обратите внимание, что такая интерпретация имеет смысл только если данные предварительно нормализованы, а также в признаках нет мультиколлинеарности (если используете обычную линейную регрессию).\n",
    "\n",
    "#### Методы\n",
    "`get_feature_importance` - метод, который вычисляет важность для каждого признака. Принимает на вход обученную линейную регрессию, возвращает список значений метрики важности признаков.\n",
    "\n",
    "`get_most_important_features`- метод, который упорядочевает признаки по их важности в порядке убывания. Принимает на вход обученную линейную регрессию, возвращает упорядоченный список, состоящий из индексов признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import get_feature_importance, get_most_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = NormalLR()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "print(get_feature_importance(regr))\n",
    "print(get_most_important_features(regr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = GradientLR(0.1, iterations=10000)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "print(get_feature_importance(regr))\n",
    "print(get_most_important_features(regr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmjSHt9rn4Ht"
   },
   "source": [
    "### Дополнительно\n",
    "Протесируйте оба метода на данных `boston.csv`, для градиентного спуска постройте график зависимости ошибки от коэффициента регуляризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk2TeC7Hn4Hu"
   },
   "outputs": [],
   "source": [
    "X, y = read_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZCb_5KVn4Hu"
   },
   "outputs": [],
   "source": [
    "regr = NormalLR()\n",
    "regr.fit(X_train, y_train)\n",
    "print(f\"MSE: {mse(y_val, y_pred)}, R2: {r2(y_test, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMkUfayxn4Hv"
   },
   "outputs": [],
   "source": [
    "build_plot(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtPBlBPkn4Hw"
   },
   "outputs": [],
   "source": [
    "regr = GradientLR(0.1, iterations=10000)\n",
    "regr.fit(X_train, y_train)\n",
    "print(f\"MSE: {mse(y_val, y_pred)}, R2: {r2(y_test, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d63Ei1yEn4Hw"
   },
   "source": [
    "Проинтерпритируйте полученные результаты. Опишите влияние каждого признака на результат предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19Ci6i5Wn4Hw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
